{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/kaylee_bae/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           statement   status\n",
      "0                                         oh my gosh  Anxiety\n",
      "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
      "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
      "3  I've shifted my focus to something else but I'...  Anxiety\n",
      "4  I'm restless and restless, it's been a month n...  Anxiety\n",
      "                                           statement   status\n",
      "0                                         oh my gosh  Anxiety\n",
      "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
      "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
      "3  I've shifted my focus to something else but I'...  Anxiety\n",
      "4  I'm restless and restless, it's been a month n...  Anxiety\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "url = './kaggle_sentiment_data.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Remove the first column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "data = data.dropna(subset=['statement', 'status'])\n",
    "\n",
    "print(data.head())\n",
    "processed_data = data[[\"statement\", \"status\"]]\n",
    "\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kaylee_bae/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kaylee_bae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kaylee_bae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/kaylee_bae/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           statement   status\n",
      "0                                     [oh, my, gosh]  Anxiety\n",
      "1  [trouble, sleep, confuse, mind, restless, hear...  Anxiety\n",
      "2  [wrong, back, off, dear, forward, doubt, stay,...  Anxiety\n",
      "3  [i, have, shift, my, focus, to, something, els...  Anxiety\n",
      "4  [i, restless, and, restless, it, be, be, a, mo...  Anxiety\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Ensure all entries in \"statement\" are strings\n",
    "processed_data[\"statement\"] = processed_data[\"statement\"].astype(str)\n",
    "\n",
    "# Define a regex pattern to match URLs\n",
    "url_pattern = re.compile(r'https?://\\S+')\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Remove URLs\n",
    "    text = url_pattern.sub('', text)\n",
    "    # Remove non-word and non-whitespace characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Define function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Convert POS tag to WordNet format\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Apply the cleaning function and tokenize in one go\n",
    "processed_data[\"statement\"] = processed_data[\"statement\"].apply(clean_text).apply(nltk.word_tokenize)\n",
    "\n",
    "# Apply lemmatization function to the tokenized column\n",
    "processed_data[\"statement\"] = processed_data[\"statement\"].apply(lemmatize_tokens)\n",
    "\n",
    "processed_data.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "print(processed_data.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs325",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

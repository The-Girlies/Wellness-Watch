{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the Naive Bayes Classifier with lemmatization as the preprocessing tool.\n",
    "\n",
    "Accuracy: 0.6617633102401063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vallipaladugu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           statement   status\n",
      "0                                         oh my gosh  Anxiety\n",
      "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
      "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
      "3  I've shifted my focus to something else but I'...  Anxiety\n",
      "4  I'm restless and restless, it's been a month n...  Anxiety\n",
      "                                           statement   status\n",
      "0                                         oh my gosh  Anxiety\n",
      "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
      "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
      "3  I've shifted my focus to something else but I'...  Anxiety\n",
      "4  I'm restless and restless, it's been a month n...  Anxiety\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "url = './kaggle_sentiment_data.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Remove the first column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "data = data.dropna(subset=['statement', 'status'])\n",
    "\n",
    "print(data.head())\n",
    "processed_data = data[[\"statement\", \"status\"]]\n",
    "\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/vallipaladugu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vallipaladugu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vallipaladugu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/vallipaladugu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           statement   status\n",
      "0                                         oh my gosh  Anxiety\n",
      "1  trouble sleep confuse mind restless heart all ...  Anxiety\n",
      "2  all wrong back off dear forward doubt stay in ...  Anxiety\n",
      "3  i have shift my focus to something else but i ...  Anxiety\n",
      "4  i be restless and restless it be be a month no...  Anxiety\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Ensure all entries in \"statement\" are strings\n",
    "processed_data[\"statement\"] = processed_data[\"statement\"].astype(str)\n",
    "\n",
    "# Define a regex pattern to match URLs\n",
    "url_pattern = re.compile(r'https?://\\S+')\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Remove URLs\n",
    "    text = url_pattern.sub('', text)\n",
    "    # Remove non-word and non-whitespace characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Define function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Convert POS tag to WordNet format\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Apply the cleaning function\n",
    "processed_data[\"statement\"] = processed_data[\"statement\"].apply(clean_text)\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "processed_data[\"statement\"] = processed_data[\"statement\"].apply(\n",
    "    lambda text: \" \".join(lemmatize_tokens(nltk.word_tokenize(text)))\n",
    ")\n",
    "\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data before augmentation\n",
    "X = processed_data['statement']\n",
    "y = processed_data['status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  \n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py:890: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py:890: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py:890: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py:890: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/naive_bayes.py:890: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for MultinomialNB\n",
    "param_grid = {\n",
    "    'alpha': [0, 0.0000001, 0.1, 0.5, 1.0, 2.0, 5.0],  # Smoothing parameter\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the GridSearchCV with MultinomialNB and the parameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    MultinomialNB(),\n",
    "    param_grid,\n",
    "    scoring='accuracy',  # Metric for evaluation\n",
    "    cv=cv,                # 5-fold cross-validation\n",
    "    verbose=1,           # Display progress\n",
    "    n_jobs=-1            # Use all available processors\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6617633102401063\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.81      0.65      0.72       755\n",
      "             Bipolar       0.84      0.45      0.59       527\n",
      "          Depression       0.49      0.80      0.61      3016\n",
      "              Normal       0.88      0.78      0.83      3308\n",
      "Personality disorder       0.92      0.15      0.26       237\n",
      "              Stress       0.77      0.16      0.27       536\n",
      "            Suicidal       0.67      0.53      0.59      2158\n",
      "\n",
      "            accuracy                           0.66     10537\n",
      "           macro avg       0.77      0.50      0.55     10537\n",
      "        weighted avg       0.72      0.66      0.66     10537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

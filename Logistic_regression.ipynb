{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normal logistic regression\n",
    "## preprocessed data import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/The-Girlies/Wellness-Watch/main/processed_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data = data.dropna(subset=['statement', 'status'])\n",
    "data_used = data[['statement', 'status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Normal): 0.9425832779728576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      7268\n",
      "           1       0.90      0.92      0.91      3269\n",
      "\n",
      "    accuracy                           0.94     10537\n",
      "   macro avg       0.93      0.94      0.93     10537\n",
      "weighted avg       0.94      0.94      0.94     10537\n",
      "\n",
      "Log-loss Cost (Normal): 0.1025131363\n"
     ]
    }
   ],
   "source": [
    "## binary: testing Normal and everything else\n",
    "\n",
    "data_used['status_value'] = (data_used['status'] == 'Normal').astype(int)\n",
    "\n",
    "# unique_status = np.unique(data['status'])\n",
    "# print(f\"Unique status values: {unique_status}\\n\")\n",
    "\n",
    "X = data_used['statement']\n",
    "y = data_used['status_value']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "word_frequency = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_frequency, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "LogisticModel = LogisticRegression()\n",
    "LogisticModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LogisticModel.predict(X_test)\n",
    "\n",
    "## Accuracy and classification report\n",
    "print(f\"Accuracy (Normal): {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Cost of logistic regression\n",
    "y_probability = LogisticModel.predict_proba(X_test)[:, 1]\n",
    "logCost = log_loss(y_pred, y_probability)\n",
    "print(f\"Log-loss Cost (Normal): {np.round(logCost, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Anxiety): 0.9615640125272849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      9769\n",
      "           1       0.87      0.55      0.68       768\n",
      "\n",
      "    accuracy                           0.96     10537\n",
      "   macro avg       0.92      0.77      0.83     10537\n",
      "weighted avg       0.96      0.96      0.96     10537\n",
      "\n",
      "Log-loss Cost (Anxiety): 0.0508027424\n"
     ]
    }
   ],
   "source": [
    "## binary: testing Anxiety and everything else\n",
    "\n",
    "data_used['status_value'] = (data_used['status'] == 'Anxiety').astype(int)\n",
    "\n",
    "X = data_used['statement']\n",
    "y = data_used['status_value']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "word_frequency = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_frequency, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "LogisticModel = LogisticRegression()\n",
    "LogisticModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LogisticModel.predict(X_test)\n",
    "\n",
    "## Accuracy and classification report\n",
    "print(f\"Accuracy (Anxiety): {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Cost of logistic regression\n",
    "y_probability = LogisticModel.predict_proba(X_test)[:, 1]\n",
    "logCost = log_loss(y_pred, y_probability)\n",
    "print(f\"Log-loss Cost (Anxiety): {np.round(logCost, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Bipolar): 0.9676378475847015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      9982\n",
      "           1       0.90      0.43      0.59       555\n",
      "\n",
      "    accuracy                           0.97     10537\n",
      "   macro avg       0.93      0.72      0.78     10537\n",
      "weighted avg       0.97      0.97      0.96     10537\n",
      "\n",
      "Log-loss Cost (Bipolar): 0.0394306238\n"
     ]
    }
   ],
   "source": [
    "## binary: testing Bipolar and everything else\n",
    "\n",
    "data_used['status_value'] = (data_used['status'] == 'Bipolar').astype(int)\n",
    "\n",
    "X = data_used['statement']\n",
    "y = data_used['status_value']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "word_frequency = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_frequency, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "LogisticModel = LogisticRegression()\n",
    "LogisticModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LogisticModel.predict(X_test)\n",
    "\n",
    "## Accuracy and classification report\n",
    "print(f\"Accuracy (Bipolar): {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Cost of logistic regression\n",
    "y_probability = LogisticModel.predict_proba(X_test)[:, 1]\n",
    "logCost = log_loss(y_pred, y_probability)\n",
    "print(f\"Log-loss Cost (Bipolar): {np.round(logCost, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Depression): 0.8177849482774984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88      7456\n",
      "           1       0.76      0.56      0.64      3081\n",
      "\n",
      "    accuracy                           0.82     10537\n",
      "   macro avg       0.80      0.74      0.76     10537\n",
      "weighted avg       0.81      0.82      0.81     10537\n",
      "\n",
      "Log-loss Cost (Depression): 0.2311239866\n"
     ]
    }
   ],
   "source": [
    "## binary: testing Depression and everything else\n",
    "\n",
    "data_used['status_value'] = (data_used['status'] == 'Depression').astype(int)\n",
    "\n",
    "X = data_used['statement']\n",
    "y = data_used['status_value']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "word_frequency = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_frequency, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "LogisticModel = LogisticRegression()\n",
    "LogisticModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LogisticModel.predict(X_test)\n",
    "\n",
    "## Accuracy and classification report\n",
    "print(f\"Accuracy (Depression): {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Cost of logistic regression\n",
    "y_probability = LogisticModel.predict_proba(X_test)[:, 1]\n",
    "logCost = log_loss(y_pred, y_probability)\n",
    "print(f\"Log-loss Cost (Depression): {np.round(logCost, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Personality disorder): 0.983296953592104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     10322\n",
      "           1       1.00      0.18      0.31       215\n",
      "\n",
      "    accuracy                           0.98     10537\n",
      "   macro avg       0.99      0.59      0.65     10537\n",
      "weighted avg       0.98      0.98      0.98     10537\n",
      "\n",
      "Log-loss Cost (Personality disorder): 0.0181866751\n"
     ]
    }
   ],
   "source": [
    "## binary: testing Personality disorder and everything else\n",
    "\n",
    "data_used['status_value'] = (data_used['status'] == 'Personality disorder').astype(int)\n",
    "\n",
    "X = data_used['statement']\n",
    "y = data_used['status_value']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "word_frequency = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_frequency, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "LogisticModel = LogisticRegression()\n",
    "LogisticModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LogisticModel.predict(X_test)\n",
    "\n",
    "## Accuracy and classification report\n",
    "print(f\"Accuracy (Personality disorder): {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Cost of logistic regression\n",
    "y_probability = LogisticModel.predict_proba(X_test)[:, 1]\n",
    "logCost = log_loss(y_pred, y_probability)\n",
    "print(f\"Log-loss Cost (Personality disorder): {np.round(logCost, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Stress): 0.9592863243807536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     10020\n",
      "           1       0.84      0.21      0.33       517\n",
      "\n",
      "    accuracy                           0.96     10537\n",
      "   macro avg       0.90      0.60      0.66     10537\n",
      "weighted avg       0.95      0.96      0.95     10537\n",
      "\n",
      "Log-loss Cost (Stress): 0.0456689442\n"
     ]
    }
   ],
   "source": [
    "## binary: testing Stress and everything else\n",
    "\n",
    "data_used['status_value'] = (data_used['status'] == 'Stress').astype(int)\n",
    "\n",
    "X = data_used['statement']\n",
    "y = data_used['status_value']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "word_frequency = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_frequency, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "LogisticModel = LogisticRegression()\n",
    "LogisticModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LogisticModel.predict(X_test)\n",
    "\n",
    "## Accuracy and classification report\n",
    "print(f\"Accuracy (Stress): {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Cost of logistic regression\n",
    "y_probability = LogisticModel.predict_proba(X_test)[:, 1]\n",
    "logCost = log_loss(y_pred, y_probability)\n",
    "print(f\"Log-loss Cost (Stress): {np.round(logCost, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Suicidal): 0.8538483439309101\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      8406\n",
      "           1       0.71      0.48      0.57      2131\n",
      "\n",
      "    accuracy                           0.85     10537\n",
      "   macro avg       0.79      0.71      0.74     10537\n",
      "weighted avg       0.84      0.85      0.84     10537\n",
      "\n",
      "Log-loss Cost (Suicidal): 0.1689629304\n"
     ]
    }
   ],
   "source": [
    "## binary: testing Suicidal and everything else\n",
    "\n",
    "data_used['status_value'] = (data_used['status'] == 'Suicidal').astype(int)\n",
    "\n",
    "X = data_used['statement']\n",
    "y = data_used['status_value']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "word_frequency = vectorizer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_frequency, y, test_size=0.2, random_state=24, stratify=y)\n",
    "\n",
    "LogisticModel = LogisticRegression()\n",
    "LogisticModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LogisticModel.predict(X_test)\n",
    "\n",
    "## Accuracy and classification report\n",
    "print(f\"Accuracy (Suicidal): {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Cost of logistic regression\n",
    "y_probability = LogisticModel.predict_proba(X_test)[:, 1]\n",
    "logCost = log_loss(y_pred, y_probability)\n",
    "print(f\"Log-loss Cost (Suicidal): {np.round(logCost, 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax (Multinomial) Accuracy: 0.7512574736642308\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.85      0.71      0.77       768\n",
      "             Bipolar       0.88      0.62      0.73       556\n",
      "          Depression       0.67      0.74      0.71      3081\n",
      "              Normal       0.84      0.95      0.89      3269\n",
      "Personality disorder       0.91      0.39      0.54       215\n",
      "              Stress       0.63      0.39      0.48       517\n",
      "            Suicidal       0.67      0.63      0.65      2131\n",
      "\n",
      "            accuracy                           0.75     10537\n",
      "           macro avg       0.78      0.63      0.68     10537\n",
      "        weighted avg       0.75      0.75      0.74     10537\n",
      "\n",
      "Softmax (Multiclass) Log-Loss (Cost): 0.6795263560032236\n"
     ]
    }
   ],
   "source": [
    "## normal logistic regression\n",
    "## preprocessed data import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/The-Girlies/Wellness-Watch/main/processed_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data = data.dropna(subset=['statement', 'status'])\n",
    "data_used = data[['statement', 'status']]\n",
    "\n",
    "## Multiclass Regression (Softmax (Multinomial Logistic Regression)) \n",
    "## multi_class = multinomial, solver = lbfgs\n",
    "\n",
    "data_soft = data[['statement', 'status']]\n",
    "\n",
    "X_soft = data_soft['statement']\n",
    "y_soft = data_soft['status']\n",
    "\n",
    "soft_vectorizer = TfidfVectorizer()\n",
    "X_soft_tfidf = soft_vectorizer.fit_transform(X)\n",
    "\n",
    "X_soft_train, X_soft_test, y_soft_train, y_soft_test = train_test_split(X_soft_tfidf, y_soft, test_size=0.2, random_state=42, stratify=y_soft)\n",
    "\n",
    "model_softmax = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, tol=1e-4)\n",
    "# report that max_iter changed from 100 to max_iter=1000\n",
    "model_softmax.fit(X_soft_train, y_soft_train)\n",
    "\n",
    "y_pred_softmax = model_softmax.predict(X_soft_test)\n",
    "\n",
    "# Accuracy for Softmax\n",
    "print(\"Softmax (Multinomial) Accuracy:\", accuracy_score(y_soft_test, y_pred_softmax))\n",
    "print(classification_report(y_soft_test, y_pred_softmax))\n",
    "\n",
    "# Cost for Softmax\n",
    "y_prob_softmax = model_softmax.predict_proba(X_soft_test)\n",
    "cost_softmax = log_loss(y_soft_test, y_prob_softmax)\n",
    "print(f'Softmax (Multiclass) Log-Loss (Cost): {cost_softmax}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax (Multinomial) Accuracy: 0.7504033406092816\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.84      0.71      0.77       768\n",
      "             Bipolar       0.88      0.62      0.73       556\n",
      "          Depression       0.67      0.74      0.71      3081\n",
      "              Normal       0.84      0.95      0.89      3269\n",
      "Personality disorder       0.91      0.38      0.54       215\n",
      "              Stress       0.62      0.39      0.48       517\n",
      "            Suicidal       0.67      0.63      0.65      2131\n",
      "\n",
      "            accuracy                           0.75     10537\n",
      "           macro avg       0.78      0.63      0.68     10537\n",
      "        weighted avg       0.75      0.75      0.74     10537\n",
      "\n",
      "Softmax (Multiclass) Log-Loss (Cost): 0.67962029677151\n"
     ]
    }
   ],
   "source": [
    "## normal logistic regression\n",
    "## preprocessed data import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/The-Girlies/Wellness-Watch/main/processed_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data = data.dropna(subset=['statement', 'status'])\n",
    "data_used = data[['statement', 'status']]\n",
    "\n",
    "## Multiclass Regression (Softmax (Multinomial Logistic Regression)) \n",
    "## multi_class = multinomial, solver = sag\n",
    "\n",
    "data_soft = data[['statement', 'status']]\n",
    "\n",
    "X_soft = data_soft['statement']\n",
    "y_soft = data_soft['status']\n",
    "\n",
    "soft_vectorizer = TfidfVectorizer()\n",
    "X_soft_tfidf = soft_vectorizer.fit_transform(X)\n",
    "\n",
    "X_soft_train, X_soft_test, y_soft_train, y_soft_test = train_test_split(X_soft_tfidf, y_soft, test_size=0.2, random_state=42, stratify=y_soft)\n",
    "\n",
    "model_softmax = LogisticRegression(multi_class='multinomial', solver='sag', max_iter=1000, tol=1e-4)\n",
    "# report that max_iter changed from 100 to max_iter=1000\n",
    "model_softmax.fit(X_soft_train, y_soft_train)\n",
    "\n",
    "y_pred_softmax = model_softmax.predict(X_soft_test)\n",
    "\n",
    "# Accuracy for Softmax\n",
    "print(\"Softmax (Multinomial) Accuracy:\", accuracy_score(y_soft_test, y_pred_softmax))\n",
    "print(classification_report(y_soft_test, y_pred_softmax))\n",
    "\n",
    "# Cost for Softmax\n",
    "y_prob_softmax = model_softmax.predict_proba(X_soft_test)\n",
    "cost_softmax = log_loss(y_soft_test, y_prob_softmax)\n",
    "print(f'Softmax (Multiclass) Log-Loss (Cost): {cost_softmax}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax (Multinomial) Accuracy: 0.7503084369365094\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.84      0.71      0.77       768\n",
      "             Bipolar       0.88      0.62      0.73       556\n",
      "          Depression       0.67      0.74      0.71      3081\n",
      "              Normal       0.84      0.95      0.89      3269\n",
      "Personality disorder       0.91      0.39      0.54       215\n",
      "              Stress       0.62      0.39      0.48       517\n",
      "            Suicidal       0.67      0.62      0.65      2131\n",
      "\n",
      "            accuracy                           0.75     10537\n",
      "           macro avg       0.78      0.63      0.68     10537\n",
      "        weighted avg       0.75      0.75      0.74     10537\n",
      "\n",
      "Softmax (Multiclass) Log-Loss (Cost): 0.679661004101361\n"
     ]
    }
   ],
   "source": [
    "## normal logistic regression\n",
    "## preprocessed data import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/The-Girlies/Wellness-Watch/main/processed_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data = data.dropna(subset=['statement', 'status'])\n",
    "data_used = data[['statement', 'status']]\n",
    "\n",
    "## Multiclass Regression (Softmax (Multinomial Logistic Regression)) \n",
    "## multi_class = multinomial, solver = saga\n",
    "\n",
    "data_soft = data[['statement', 'status']]\n",
    "\n",
    "X_soft = data_soft['statement']\n",
    "y_soft = data_soft['status']\n",
    "\n",
    "soft_vectorizer = TfidfVectorizer()\n",
    "X_soft_tfidf = soft_vectorizer.fit_transform(X)\n",
    "\n",
    "X_soft_train, X_soft_test, y_soft_train, y_soft_test = train_test_split(X_soft_tfidf, y_soft, test_size=0.2, random_state=42, stratify=y_soft)\n",
    "\n",
    "model_softmax = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000, tol=1e-4)\n",
    "# report that max_iter changed from 100 to max_iter=1000\n",
    "model_softmax.fit(X_soft_train, y_soft_train)\n",
    "\n",
    "y_pred_softmax = model_softmax.predict(X_soft_test)\n",
    "\n",
    "# Accuracy for Softmax\n",
    "print(\"Softmax (Multinomial) Accuracy:\", accuracy_score(y_soft_test, y_pred_softmax))\n",
    "print(classification_report(y_soft_test, y_pred_softmax))\n",
    "\n",
    "# Cost for Softmax\n",
    "y_prob_softmax = model_softmax.predict_proba(X_soft_test)\n",
    "cost_softmax = log_loss(y_soft_test, y_prob_softmax)\n",
    "print(f'Softmax (Multiclass) Log-Loss (Cost): {cost_softmax}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax (Multinomial) Accuracy: 0.7498339185726488\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.84      0.71      0.77       768\n",
      "             Bipolar       0.88      0.62      0.73       556\n",
      "          Depression       0.67      0.74      0.71      3081\n",
      "              Normal       0.84      0.95      0.89      3269\n",
      "Personality disorder       0.91      0.38      0.54       215\n",
      "              Stress       0.63      0.39      0.48       517\n",
      "            Suicidal       0.67      0.62      0.65      2131\n",
      "\n",
      "            accuracy                           0.75     10537\n",
      "           macro avg       0.78      0.63      0.68     10537\n",
      "        weighted avg       0.75      0.75      0.74     10537\n",
      "\n",
      "Softmax (Multiclass) Log-Loss (Cost): 0.6797596909604097\n"
     ]
    }
   ],
   "source": [
    "## normal logistic regression\n",
    "## preprocessed data import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/The-Girlies/Wellness-Watch/main/processed_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data = data.dropna(subset=['statement', 'status'])\n",
    "data_used = data[['statement', 'status']]\n",
    "\n",
    "## Multiclass Regression (Softmax (Multinomial Logistic Regression)) \n",
    "## multi_class = multinomial, solver = newton-cg\n",
    "\n",
    "data_soft = data[['statement', 'status']]\n",
    "\n",
    "X_soft = data_soft['statement']\n",
    "y_soft = data_soft['status']\n",
    "\n",
    "soft_vectorizer = TfidfVectorizer()\n",
    "X_soft_tfidf = soft_vectorizer.fit_transform(X)\n",
    "\n",
    "X_soft_train, X_soft_test, y_soft_train, y_soft_test = train_test_split(X_soft_tfidf, y_soft, test_size=0.2, random_state=42, stratify=y_soft)\n",
    "\n",
    "model_softmax = LogisticRegression(multi_class='multinomial', solver='newton-cg', max_iter=1000, tol=1e-4)\n",
    "# report that max_iter changed from 100 to max_iter=1000\n",
    "model_softmax.fit(X_soft_train, y_soft_train)\n",
    "\n",
    "y_pred_softmax = model_softmax.predict(X_soft_test)\n",
    "\n",
    "# Accuracy for Softmax\n",
    "print(\"Softmax (Multinomial) Accuracy:\", accuracy_score(y_soft_test, y_pred_softmax))\n",
    "print(classification_report(y_soft_test, y_pred_softmax))\n",
    "\n",
    "# Cost for Softmax\n",
    "y_prob_softmax = model_softmax.predict_proba(X_soft_test)\n",
    "cost_softmax = log_loss(y_soft_test, y_prob_softmax)\n",
    "print(f'Softmax (Multiclass) Log-Loss (Cost): {cost_softmax}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
